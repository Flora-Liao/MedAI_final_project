{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f95dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549f80ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(base_path):\n",
    "    \"\"\"\n",
    "    Load image paths and corresponding YOLO annotation files.\n",
    "    \n",
    "    Args:\n",
    "        base_path (str): Path to the parent folder containing 'benign_yolo' and 'malignant_yolo'.\n",
    "        \n",
    "    Returns:\n",
    "        list of tuples: Each tuple contains (img_path, label_path, class_label).\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for class_folder in ['benign_yolo', 'malignant_yolo']:\n",
    "        dir_path = os.path.join(base_path, class_folder)\n",
    "        class_label = class_folder.split('_')[0]  # 'benign' or 'malignant'\n",
    "        \n",
    "        if not os.path.isdir(dir_path):\n",
    "            continue\n",
    "        \n",
    "        for fname in os.listdir(dir_path):\n",
    "            if fname.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                img_path = os.path.join(dir_path, fname)\n",
    "                base, _ = os.path.splitext(img_path)\n",
    "                if base.endswith('_denoised'):\n",
    "                    base = base[: -len('_denoised')]\n",
    "                txt_path = base + '.txt'\n",
    "                if os.path.exists(txt_path):\n",
    "                    data.append((img_path, txt_path, class_label))\n",
    "    return data\n",
    "\n",
    "def read_yolo_annotation(txt_path, img_shape):\n",
    "    \"\"\"\n",
    "    Read a YOLO annotation file and convert to pixel coordinates.\n",
    "    \n",
    "    Args:\n",
    "        txt_path (str): Path to the YOLO .txt file.\n",
    "        img_shape (tuple): Shape of the image (height, width).\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (x, y, w, h) pixel coordinates of the bounding box.\n",
    "    \"\"\"\n",
    "    h_img, w_img = img_shape[:2]\n",
    "    boxes = []\n",
    "\n",
    "    with open(txt_path, \"r\") as f:\n",
    "        for line in f:                       # <- stay inside this loop\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            parts = line.split()\n",
    "            if len(parts) != 5:\n",
    "                continue\n",
    "\n",
    "            # ↓ THESE FOUR LINES MUST BE INSIDE THE for-loop\n",
    "            _, x_c, y_c, w, h = map(float, parts)\n",
    "            x_c *= w_img;  y_c *= h_img\n",
    "            w   *= w_img;  h *= h_img\n",
    "            x1  = int(x_c - w / 2);  y1 = int(y_c - h / 2)\n",
    "            boxes.append((x1, y1, int(w), int(h)))\n",
    "\n",
    "    return boxes\n",
    "\n",
    "def visualize_samples(data, n=5, seed=42):\n",
    "    \"\"\"\n",
    "    Randomly sample and display images with bounding boxes.\n",
    "    \n",
    "    Args:\n",
    "        data (list): List of (img_path, txt_path, class_label) tuples.\n",
    "        n (int): Number of samples to display.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    samples = random.sample(data, min(n, len(data)))\n",
    "    for img_path, txt_path, label in samples:\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            continue\n",
    "        \n",
    "        for (x, y, w, h) in read_yolo_annotation(txt_path, img.shape):\n",
    "            cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.putText(img, label, (x, max(0, y - 10)),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da741636",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = r'D:\\datasets_in_D\\Data_Science_for_Digital_Health\\QAMEBI_CLAHE'\n",
    "    \n",
    "data = load_dataset(base_path)\n",
    "print(f\"Loaded {len(data)} annotated images.\")\n",
    "    \n",
    "# Visualize a few samples (adjust n as needed)\n",
    "visualize_samples(data, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5cb918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_and_save_images(data, output_path):\n",
    "    cropped = []\n",
    "    for img_path, txt_path, label in data:\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            continue\n",
    "\n",
    "        boxes = read_yolo_annotation(txt_path, img.shape)\n",
    "        if not boxes:\n",
    "            continue\n",
    "\n",
    "        out_dir = os.path.join(output_path, label)\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        base = os.path.splitext(os.path.basename(img_path))[0]\n",
    "\n",
    "        for idx, (x, y, w, h) in enumerate(boxes):\n",
    "            crop = img[y:y+h, x:x+w]\n",
    "            if crop.size == 0:\n",
    "                continue\n",
    "\n",
    "            # ↓ THESE LINES BELONG INSIDE the for-loop\n",
    "            crop_name = f\"{base}_crop{idx}.png\"\n",
    "            crop_path = os.path.join(out_dir, crop_name)\n",
    "            cv2.imwrite(crop_path, crop)\n",
    "            cropped.append((crop_path, txt_path, label))\n",
    "\n",
    "    return cropped\n",
    "\n",
    "def visualize_crops(cropped, n=5, seed=42):\n",
    "    \"\"\"\n",
    "    Randomly sample and display cropped images to verify.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    samples = random.sample(cropped, min(n, len(cropped)))\n",
    "    for img_path, txt_path, label in samples:\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            continue\n",
    "        for (x, y, w, h) in read_yolo_annotation(txt_path, img.shape):\n",
    "            cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            \n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        plt.figure(figsize=(6,6))\n",
    "        plt.imshow(img_rgb)\n",
    "        plt.title(label)\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8337c7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = r'D:\\datasets_in_D\\Data_Science_for_Digital_Health\\QAMEBI_CLAHE\\cropped_imgs'\n",
    "       \n",
    "# 1) crop & save\n",
    "cropped = crop_and_save_images(data, output_path)\n",
    "print(f\"Cropped & saved {len(cropped)} images to '{output_path}'.\")\n",
    "\n",
    "# 2) visualize random crops\n",
    "visualize_crops(cropped, n=5, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b3d9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_min_dims(cropped):\n",
    "    \"\"\"\n",
    "    Compute the minimum width and height among all cropped images.\n",
    "    \"\"\"\n",
    "    min_w, min_h = float('inf'), float('inf')\n",
    "    max_w, max_h = 0, 0\n",
    "    for crop_path, _, _ in cropped:\n",
    "        img = cv2.imread(crop_path)\n",
    "        if img is None:\n",
    "            continue\n",
    "        h, w = img.shape[:2]\n",
    "        if w < min_w:\n",
    "            min_w = w\n",
    "        if h < min_h:\n",
    "            min_h = h\n",
    "        if w > max_w:\n",
    "            max_w = w\n",
    "        if h > max_h:\n",
    "            max_h = h\n",
    "    return int(min_w), int(min_h), int(max_w), int(max_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbfe72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_w, min_h, max_w, max_h = find_max_min_dims(cropped)\n",
    "print(f\"Minimum crop width: {min_w} pixels\")\n",
    "print(f\"Minimum crop height: {min_h} pixels\")\n",
    "print(f\"Maximum crop width: {max_w} pixels\")\n",
    "print(f\"Maximum crop height: {max_h} pixels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7643bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_to_square(img):\n",
    "    h, w = img.shape[:2]\n",
    "    diff = abs(h - w)\n",
    "    pad1, pad2 = diff // 2, diff - diff // 2\n",
    "    if h < w:\n",
    "        # pad top and bottom\n",
    "        return cv2.copyMakeBorder(img, pad1, pad2, 0, 0,\n",
    "                                  borderType=cv2.BORDER_CONSTANT,\n",
    "                                  value=[0, 0, 0])\n",
    "    else:\n",
    "        # pad left and right\n",
    "        return cv2.copyMakeBorder(img, 0, 0, pad1, pad2,\n",
    "                                  borderType=cv2.BORDER_CONSTANT,\n",
    "                                  value=[0, 0, 0])\n",
    "\n",
    "def preprocess_crop(img, target_size=224):\n",
    "    square = pad_to_square(img)\n",
    "    return cv2.resize(square, (target_size, target_size), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "def visualize_actual_size_gray(samples, title):\n",
    "    \"\"\"\n",
    "    Same as visualize_actual_size, but first convert to true grayscale\n",
    "    and show with a gray colormap.\n",
    "    \"\"\"\n",
    "    for info in samples:\n",
    "        # load the original crop (still 3-channel BGR)\n",
    "        orig_bgr = cv2.imread(info['path'])\n",
    "        # convert both original & processed to gray\n",
    "        orig_gray = cv2.cvtColor(orig_bgr, cv2.COLOR_BGR2GRAY)\n",
    "        proc_bgr = preprocess_crop(orig_bgr)    # from earlier code\n",
    "        proc_gray = cv2.cvtColor(proc_bgr, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        oh, ow = orig_gray.shape\n",
    "        ph, pw = proc_gray.shape  # should be 224×224\n",
    "        \n",
    "        # figure size so 1px ≃ 1 displaypx\n",
    "        fig = plt.figure(\n",
    "            figsize=((ow + pw)/100, max(oh, ph)/100),\n",
    "            dpi=100\n",
    "        )\n",
    "        gs = fig.add_gridspec(1, 2, width_ratios=[ow, pw])\n",
    "        ax0 = fig.add_subplot(gs[0, 0])\n",
    "        ax1 = fig.add_subplot(gs[0, 1])\n",
    "        \n",
    "        ax0.imshow(orig_gray, cmap='gray')\n",
    "        ax0.set_title(f\"{ow}×{oh}\")\n",
    "        ax0.axis('off');  ax0.set_aspect('equal')\n",
    "        \n",
    "        ax1.imshow(proc_gray, cmap='gray')\n",
    "        ax1.set_title(f\"{pw}×{ph}\")\n",
    "        ax1.axis('off');  ax1.set_aspect('equal')\n",
    "        \n",
    "        plt.suptitle(f\"{title}: {os.path.basename(info['path'])}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9700b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) gather dimensions\n",
    "crops_info = []\n",
    "for path, _, label in cropped:\n",
    "    img = cv2.imread(path)\n",
    "    if img is None: continue\n",
    "    h, w = img.shape[:2]\n",
    "    crops_info.append({'path': path, 'label': label, 'h': h, 'w': w, 'max_dim': max(h, w)})\n",
    "\n",
    "# sort by max dimension\n",
    "sorted_crops = sorted(crops_info, key=lambda x: x['max_dim'])\n",
    "smallest = sorted_crops[0]    # should be 40×40\n",
    "largest  = sorted_crops[-1]   # max ~540×424\n",
    "\n",
    "# seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# select upsample (max_dim < 224)\n",
    "up_list = [c for c in sorted_crops if c['max_dim'] < 224]\n",
    "up_samples = [smallest] + random.sample([c for c in up_list if c != smallest], min(4, len(up_list)-1))\n",
    "\n",
    "# select downscale (max_dim > 224)\n",
    "down_list = [c for c in sorted_crops if c['max_dim'] > 224]\n",
    "down_samples = [largest] + random.sample([c for c in down_list if c != largest], min(4, len(down_list)-1))\n",
    "\n",
    "# 3) visualize and save\n",
    "save_gray_path = r\"D:\\datasets_in_D\\Data_Science_for_Digital_Health\\QAMEBI_CLAHE\\processed\"\n",
    "visualize_actual_size_gray(up_samples, 'Upsampled (<224)')\n",
    "visualize_actual_size_gray(down_samples, 'Downscaled (>224)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98613d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_preprocessed_grayscale_images(cropped_info, save_dir, target_size=224):\n",
    "    \"\"\"\n",
    "    Preprocess (pad, resize, convert to grayscale) and save images\n",
    "    in save_dir/<label>/ as 'label (#).png'.\n",
    "    \n",
    "    Args:\n",
    "        cropped_info (list): List of dicts with 'path' and 'label'.\n",
    "        save_dir (str): Base directory to save images.\n",
    "        target_size (int): Output size for square image.\n",
    "    \"\"\"\n",
    "    label_counters = {'benign': 1, 'malignant': 1}\n",
    "\n",
    "    for info in cropped_info:\n",
    "        img = cv2.imread(info['path'])\n",
    "        if img is None:\n",
    "            continue\n",
    "        \n",
    "        # Pad to square and resize\n",
    "        processed = preprocess_crop(img, target_size=target_size)\n",
    "        # Convert to grayscale\n",
    "        gray = cv2.cvtColor(processed, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        label = info['label']\n",
    "        label_folder = os.path.join(save_dir, label)\n",
    "        os.makedirs(label_folder, exist_ok=True)\n",
    "        \n",
    "        # Generate filename like 'benign (1).png'\n",
    "        count = label_counters[label]\n",
    "        filename = f\"{label} ({count}).png\"\n",
    "        full_path = os.path.join(label_folder, filename)\n",
    "        \n",
    "        # Save the grayscale image\n",
    "        cv2.imwrite(full_path, gray)\n",
    "        label_counters[label] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7647ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_preprocessed_grayscale_images(crops_info, save_gray_path, target_size=224)\n",
    "print(\"Preprocessed grayscale images saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataScience_DigitalHealth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
